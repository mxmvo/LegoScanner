{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import rpyc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import cma\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.system import System\n",
    "from environment.environment import Environment\n",
    "\n",
    "from agent.tabular_q_learning import Agent as T_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- 0 (rotation), 1 (other rotation), 2 (move outwards), 3 (move inwards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "\n",
    "The agent takes as input a vector/matrix and output a probability distribution\n",
    "\n",
    "The action is taken using an argmax. Then reward is 1 or 0 then from the reward get the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_steps, num_features, num_actions, num_hidden = 5):\n",
    "        super(Agent, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_steps*num_features, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.softmax(self.layer2(x))\n",
    "        return x\n",
    "      \n",
    "      \n",
    "    def set_weights(self, weights):\n",
    "        assert np.prod(weights.shape) == self.num_parameters, \"Number of weights do not coincide with number of parameters\"\n",
    "        weights = weights.reshape(-1)\n",
    "        count = 0\n",
    "        for l in self.parameters():\n",
    "            l.data = torch.FloatTensor(weights[count:count+np.prod(l.size())]).view(l.size())\n",
    "            count += np.prod(l.size())\n",
    "        \n",
    "      \n",
    "    @property\n",
    "    def num_parameters(self):\n",
    "        count = 0\n",
    "        for l in self.parameters():\n",
    "            count += np.prod(l.size())\n",
    "        return count\n",
    "        \n",
    "      \n",
    "criterion = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensors': {'bot': <environment.system.EnvSensor object at 0x111ddfb70>, 'top': <environment.system.EnvSensor object at 0x111e1c390>, 'cs': <environment.system.EnvSensor object at 0x111e1c400>}, 'actionables': {'bot': <environment.system.EnvActionable object at 0x111ddfcc0>, 'top': <environment.system.EnvActionable object at 0x111ddfe80>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def find_line_state(state_queue):\n",
    "    s = np.array(state_queue)[-3:,-2:]\n",
    "    return torch.FloatTensor(s).view(-1)\n",
    "  \n",
    "def find_line_reward(reward_queue):\n",
    "    r = np.array(reward_queue)[-5:].mean()\n",
    "    if r > 1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "  \n",
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', \n",
    "                  state_queue_len = 10, \n",
    "                  get_reward_function = find_line_reward,\n",
    "                  get_state_function = find_line_state)\n",
    "\n",
    "agent = Agent(num_steps = 3, num_features = 2, num_actions = 16, num_hidden = 5)\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(),lr= .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.rand(131)\n",
    "agent.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(x, agent, env):\n",
    "    agent.set_weights(x)\n",
    "    \n",
    "    env.reset()\n",
    "    state = env.state\n",
    "    for i in range(30):\n",
    "        action_prob = agent(state)\n",
    "    \n",
    "        action = torch.multinomial(action_prob, 1).detach().numpy()[0]\n",
    "        stats.append(action)\n",
    "        state, rew, _, _ = env.step([action//2,action%2])\n",
    "        pred.append(action_prob[action])\n",
    "\n",
    "        true.append(rew)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do the learning\n",
    "for i in range(tot_runs):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        sess.graph.finalize()  # graph is read-only after this statement\n",
    "        initial_weights = np.random.normal(0, 0.1, number_of_trainable_parameters(sess))\n",
    "        res = cma.fmin(fitness_cart_pole, initial_weights, 1, {'maxfevals': 5000, 'ftarget':-199.9,}, args=([sess, env]))\n",
    "      \n",
    "    results[i,0] = res[4]\n",
    "    # Evaluate the solution.\n",
    "    Rs = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        sess.graph.finalize()  # graph is read-only after this statement\n",
    "        for _ in range(eval_runs):\n",
    "          Rs += run_cart_pole(res[0],sess,env)\n",
    "        env.close()    \n",
    "      \n",
    "    results[i,1] = Rs/eval_runs\n",
    "    end_results_no_bias[j]= results.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6])\n",
      "torch.Size([5])\n",
      "torch.Size([16, 5])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i in agent.parameters():\n",
    "    print(i.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training:\n",
    "# Input for this classifier: previous 3 positions (x_1, y_1, x_2, y_2, x_3, y_3) output get action\n",
    "# each 10 steps update the weights\n",
    "# Decide when the action gives 1 for right and 0 for wrong (so a 1-0 reward)\n",
    "# Then use the Batch Cross Entropy Loss\n",
    "# Tadamorrow\n",
    "\n",
    "train_steps = 50\n",
    "batch_size = 20\n",
    "\n",
    "env.reset()\n",
    "state = env.state\n",
    "for i in range(train_steps):\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "    stats = []\n",
    "    for j in range(batch_size):\n",
    "        action_prob = agent(state)\n",
    "    \n",
    "        action = torch.multinomial(action_prob, 1).detach().numpy()[0]\n",
    "        stats.append(action)\n",
    "        state, rew, _, _ = env.step([action//2,action%2])\n",
    "        pred.append(action_prob[action])\n",
    "\n",
    "        true.append(rew)\n",
    "\n",
    "\n",
    "    # Plot some stuff\n",
    "    fig, ax = plt.subplots(ncols = 2)\n",
    "    \n",
    "    ax[0].hist(stats)\n",
    "    ax[0].set_title('actions')\n",
    "    \n",
    "    ax[1].plot(true)\n",
    "    ax[1].set_title('rewards')\n",
    "    \n",
    "    plt.show()\n",
    "    pred = torch.stack(pred)\n",
    "    true = torch.FloatTensor(true)\n",
    "    loss = criterion(pred, true)\n",
    "    agent.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "env.go_to_init_state()\n",
    "        \n",
    "        \n",
    "  \n",
    "\n",
    "#loss = criterion(X,Y)\n",
    "#model.zero_grad()\n",
    "#loss.backward()\n",
    "#optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.go_to_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [torch.rand(1) for _ in range(10)]\n",
    "print(torch.tensor(x))\n",
    "y = torch.randint(2,size = (10,))\n",
    "x/x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.multinomial(x/x.sum(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_np, y_np = x.numpy, y.numpy\n",
    "\n",
    "(y*np.log(x) +(1-y)*np.log(1-x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(num_steps = 3, features = 4, num_actions = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rand = torch.rand(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 30\n",
    "\n",
    "# Make an Agent\n",
    "\n",
    "q_table = T_Agent(4, learn_rate = .8, gamma =.95)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "stop_flag = False\n",
    "for i in range(num_episodes):\n",
    "    # Decay the exploration\n",
    "    q_table.explore_decay = i\n",
    "    \n",
    "    s = env.go_to_init_state()\n",
    "    rAll = 0\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    try:\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = q_table.next_action(s)\n",
    "            print('Action',a)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            print('\\r   ', r)\n",
    "\n",
    "            #Update Q-Table with new knowledge\n",
    "            q_table.update(r, s1)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "#         stop_flag = True\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "    rewards.append(rAll)\n",
    "    print('#'*10, 'End Episode', '#'*10)\n",
    "    \n",
    "print(\"Average score over last part \" +  str(sum(rewards[-500:])/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(env.state)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_table.val_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

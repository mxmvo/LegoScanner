{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import rpyc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.system import System\n",
    "from environment.environment import Environment\n",
    "\n",
    "from agent.tabular_q_learning import Agent as T_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- 0 (rotation), 1 (other rotation), 2 (move outwards), 3 (move inwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, field_classifier, reward_classifier, get_reward_function, get_state_function, state_queue_len=10):\n",
    "        self.system = System(brick_ip='ev3dev.local', get_state_mode='dict')\n",
    "        self.field_classifier = utils.load_pickle(field_classifier)\n",
    "        self.reward_classifier = utils.load_pickle(reward_classifier)\n",
    "        self.opposite_action = {0:3,1:2,2:1,3:0}\n",
    "        self.opposite_action = {i: self.action_space-i-1 for i in range(self.action_space)}\n",
    "        \n",
    "        self.on_field = True\n",
    "        self.border_count = 0\n",
    "#         self.color_on = color_on\n",
    "        \n",
    "        self.state_queue = collections.deque(maxlen=state_queue_len)\n",
    "        self.reward_queue = collections.deque(maxlen=state_queue_len)\n",
    "        \n",
    "        self.get_reward_function = get_reward_function\n",
    "        self.get_state_function = get_state_function\n",
    "        \n",
    "        for _ in range(state_queue_len):\n",
    "            self._new_state()\n",
    "        \n",
    "    def reset(self):\n",
    "        # stop current action\n",
    "        self.system.reset()\n",
    "        # Go to initial state\n",
    "\n",
    "        # return state\n",
    "#         return self.prepro([self.state])\n",
    "      \n",
    "    def go_to_init_state(self):\n",
    "        print('#'*30)\n",
    "        print('Going to Init')\n",
    "        print('#'*30)\n",
    "        self.system.go_to_init_state()\n",
    "#         time.sleep(3)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _color_from_one_state(s):\n",
    "        return s[:3]\n",
    "        \n",
    "    def _environment_checks(self):\n",
    "        # access color information from the last measurement \n",
    "        # according to self.new_state() ordering\n",
    "        color = self._color_state_for_classifier\n",
    "        \n",
    "        if self.field_classifier.predict(color) == [0]:\n",
    "            print('I am outside')\n",
    "            self.border_count += 1\n",
    "\n",
    "            if self.on_field:\n",
    "                self.system.perform_actions([self.opposite_action[a] for a in self.current_action])\n",
    "                print('BOUNCIN!!1')\n",
    "                time.sleep(1)\n",
    "            self.on_field = False\n",
    "        else:\n",
    "            self.on_field = True\n",
    "    \n",
    "        if self.border_count == 3:\n",
    "            self.go_to_init_state()\n",
    "            self.border_count = 0\n",
    "            \n",
    "    @property\n",
    "    def _color_state_for_classifier(self):\n",
    "        return np.array([self._color_from_one_state(s) for s in list(self.state_queue)[-2:]]).reshape(1,-1)\n",
    "            \n",
    "    def _new_reward_approx(self):\n",
    "        def transform_proba_into_reward_approx(proba):\n",
    "            return np.max([0., 5. * (proba - 0.3)])\n",
    "            \n",
    "        # Predict propba\n",
    "        if not self.on_field:\n",
    "            return -10\n",
    "        \n",
    "        colors = self._color_state_for_classifier\n",
    "#         r = (np.argmax(self.reward_classifier.predict_proba(x), axis = 1) == 1).sum()\n",
    "        # sum the probabilities of black class and compute a function of it\n",
    "        black_proba = self.reward_classifier.predict_proba(colors)[:,1][0]\n",
    "        \n",
    "        self.reward_queue.append(transform_proba_into_reward_approx(black_proba))\n",
    "        \n",
    "    def _new_state(self):\n",
    "        s = self.system.get_state()\n",
    "        color = s['cs'][0]\n",
    "        top_pos = s['top'][0]\n",
    "        bot_pos = s['bot'][0]\n",
    "        self.state_queue.append((*color, top_pos, bot_pos))\n",
    "        \n",
    "    def _cycle(self, is_free_cycle):\n",
    "        if not is_free_cycle:\n",
    "            print(\"Performing action\", self.current_action)\n",
    "            self.system.perform_actions(self.current_action)\n",
    "        \n",
    "        # gets new states and puts it in the queue\n",
    "        self._new_state()\n",
    "        \n",
    "        # environment specific checks like is it still in the field\n",
    "        self._environment_checks()\n",
    "        \n",
    "        # calculate the reward \n",
    "        self._new_reward_approx()\n",
    "        \n",
    "    def step(self, action, free_cycles=5):\n",
    "        self.current_action = action\n",
    "        self._cycle(False)  \n",
    "        for _ in range(free_cycles):\n",
    "            self._cycle(True)\n",
    "        return self.state, self.reward, False, {}\n",
    "    \n",
    "    @property\n",
    "    def reward(self):\n",
    "        return self.get_reward_function(self.reward_queue)\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.get_state_function(self.state_queue)\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self.system.get_action_space()[0])\n",
    "\n",
    "def get_reward_function(reward_queue):\n",
    "    # how many last reward_approx to take into consideration\n",
    "    rewards = list(reward_queue)[-3:]\n",
    "    \n",
    "    # linear weights\n",
    "    weights = np.ones(shape=(len(rewards),))\n",
    "    weights = [weight * i for i, weight in enumerate(weights)]\n",
    "    rewards_weighted = [weight * p for weight, p in zip(weights, rewards)]\n",
    "    \n",
    "    # custom function deciding what should be rewarded (because reward_approx is based on probability)\n",
    "#     black_threshold = 0.3\n",
    "#     r = np.max([0, (np.sum(rewards_weighted)-(black_threshold*len(rewards_weighted))) * 5])\n",
    "    \n",
    "    return np.mean(rewards_weighted)\n",
    "    \n",
    "def get_state_function(state_queue):\n",
    "#     print(state_queue)\n",
    "    state = list(state_queue)\n",
    "    s = state[-5:]\n",
    "    return np.array(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', \n",
    "                  state_queue_len = 10, \n",
    "                  get_reward_function = get_reward_function,\n",
    "                  get_state_function = get_state_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.go_to_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step([0,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(100):\n",
    "    a = np.random.randint(env.action_space, size = 2)\n",
    "    print(env.step(list(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.system.perform_actions([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reward_queue, env.state_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.state_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "\n",
    "The agent takes as input a vector/matrix and output a probability distribution\n",
    "\n",
    "The action is taken using an argmax. Then reward is 1 or 0 then from the reward get the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_steps, num_features, num_actions, num_hidden = 5):\n",
    "        super(Agent, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_steps*num_features, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.softmax(self.layer2(x))\n",
    "        return x\n",
    "      \n",
    "      \n",
    "      \n",
    "criterion = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensors': {'bot': <environment.system.EnvSensor object at 0x111e175f8>, 'top': <environment.system.EnvSensor object at 0x111e0ad68>, 'cs': <environment.system.EnvSensor object at 0x111e0ada0>}, 'actionables': {'bot': <environment.system.EnvActionable object at 0x111e175c0>, 'top': <environment.system.EnvActionable object at 0x111e17630>}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.20.0 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def find_line_state(state_queue):\n",
    "    s = np.array(state_queue)[-3:,-2:]\n",
    "    return torch.FloatTensor(s).view(-1)\n",
    "  \n",
    "def find_line_reward(reward_queue):\n",
    "    r = np.array(reward_queue)[-5:].mean()\n",
    "    if r > 1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "  \n",
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', \n",
    "                  state_queue_len = 10, \n",
    "                  get_reward_function = find_line_reward,\n",
    "                  get_state_function = find_line_state)\n",
    "\n",
    "agent = Agent(num_steps = 3, num_features = 2, num_actions = 16, num_hidden = 5)\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(),lr= .01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training:\n",
    "# Input for this classifier: previous 3 positions (x_1, y_1, x_2, y_2, x_3, y_3) output get action\n",
    "# each 10 steps update the weights\n",
    "# Decide when the action gives 1 for right and 0 for wrong (so a 1-0 reward)\n",
    "# Then use the Batch Cross Entropy Loss\n",
    "# Tadamorrow\n",
    "\n",
    "train_steps = 50\n",
    "batch_size = 20\n",
    "\n",
    "env.reset()\n",
    "state = env.state\n",
    "for i in range(train_steps):\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "    stats = []\n",
    "    for j in range(batch_size):\n",
    "        action_prob = agent(state)\n",
    "    \n",
    "        action = torch.multinomial(action_prob, 1).detach().numpy()[0]\n",
    "        stats.append(action)\n",
    "        state, rew, _, _ = env.step([action//2,action%2])\n",
    "        pred.append(action_prob[action])\n",
    "\n",
    "        true.append(rew)\n",
    "\n",
    "\n",
    "    # Plot some stuff\n",
    "    fig, ax = plt.subplots(ncols = 2)\n",
    "    \n",
    "    ax[0].hist(stats)\n",
    "    ax[0].set_title('actions')\n",
    "    \n",
    "    ax[1].plot(true)\n",
    "    ax[1].set_title('rewards')\n",
    "    \n",
    "    plt.show()\n",
    "    pred = torch.stack(pred)\n",
    "    true = torch.FloatTensor(true)\n",
    "    loss = criterion(pred, true)\n",
    "    agent.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "env.go_to_init_state()\n",
    "        \n",
    "        \n",
    "  \n",
    "\n",
    "#loss = criterion(X,Y)\n",
    "#model.zero_grad()\n",
    "#loss.backward()\n",
    "#optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.go_to_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [torch.rand(1) for _ in range(10)]\n",
    "print(torch.tensor(x))\n",
    "y = torch.randint(2,size = (10,))\n",
    "x/x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.multinomial(x/x.sum(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_np, y_np = x.numpy, y.numpy\n",
    "\n",
    "(y*np.log(x) +(1-y)*np.log(1-x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(num_steps = 3, features = 4, num_actions = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_rand = torch.rand(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 30\n",
    "\n",
    "# Make an Agent\n",
    "\n",
    "q_table = T_Agent(4, learn_rate = .8, gamma =.95)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "stop_flag = False\n",
    "for i in range(num_episodes):\n",
    "    # Decay the exploration\n",
    "    q_table.explore_decay = i\n",
    "    \n",
    "    s = env.go_to_init_state()\n",
    "    rAll = 0\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    try:\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = q_table.next_action(s)\n",
    "            print('Action',a)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            print('\\r   ', r)\n",
    "\n",
    "            #Update Q-Table with new knowledge\n",
    "            q_table.update(r, s1)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "#         stop_flag = True\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "    rewards.append(rAll)\n",
    "    print('#'*10, 'End Episode', '#'*10)\n",
    "    \n",
    "print(\"Average score over last part \" +  str(sum(rewards[-500:])/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(env.state)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_table.val_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

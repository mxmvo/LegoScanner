{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpyc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.system import System\n",
    "\n",
    "from agent.tabular_q_learning import Agent as T_Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- 0 (rotation), 1 (other rotation), 2 (move outwards), 3 (move inwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, field_classifier, reward_classifier, delta_measurement = .05, num_measurements = 10, color_on = True):\n",
    "        self.__env = System(brick_ip='ev3dev.local', get_state_mode='dict')\n",
    "        self.delta_measurement = delta_measurement\n",
    "        self.num_measurements = num_measurements\n",
    "        self.field_classifier = utils.load_pickle(field_classifier)\n",
    "        self.reward_classifier = utils.load_pickle(reward_classifier)\n",
    "        self.opposite_action = {0:1,1:0,2:3,3:2}\n",
    "        \n",
    "        self.on_field = True\n",
    "        self.color_on = color_on\n",
    "        \n",
    "    def reset(self):\n",
    "        # stop current action\n",
    "        self.__env.reset()\n",
    "        # Go to initial state\n",
    "\n",
    "        # return state\n",
    "        return self.prepro([self.state])\n",
    "      \n",
    "    def go_to_init_state(self):\n",
    "        self.__env.go_to_init_state()\n",
    "        print('#'*30)\n",
    "        print('Going to Init')\n",
    "        print('#'*30)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def step(self, action):\n",
    "        # give the action to the motors\n",
    "        self.__env.perform_actions([action])\n",
    "        \n",
    "        state = []\n",
    "        done = False\n",
    "        \n",
    "        # we will perform this action for \n",
    "        measurement = 0\n",
    "        \n",
    "        border_count = 0\n",
    "        \n",
    "        while measurement < self.num_measurements:\n",
    "            start = time.time()\n",
    "            time_arr = []\n",
    "            # Get the current state\n",
    "            s = self.get_state()\n",
    "            state.append(s)\n",
    "            start_1 = time.time()\n",
    "            time_arr.append(start_1-start)\n",
    "            \n",
    "            measurement += 1\n",
    "            \n",
    "            #Sleep a bit so next time we get a different state\n",
    "            time.sleep(self.delta_measurement)\n",
    "            start_2 = time.time()\n",
    "            time_arr.append(start_2-start_1)\n",
    "            \n",
    "            # A check whether we are still in the field\n",
    "            if self.color_on:\n",
    "                if self.field_classifier.predict(s['raw_col']) == [0]:\n",
    "                    print('I am outside')\n",
    "                    border_count += 1\n",
    "            \n",
    "                    if self.on_field:\n",
    "                        self.__env.perform_actions([self.opposite_action[action]])\n",
    "                        print('BOUNCIN!!1')\n",
    "                        time.sleep(1)\n",
    "                    self.on_field = False\n",
    "                else:\n",
    "                    self.on_field = True\n",
    "            \n",
    "            if border_count ==3:\n",
    "                self.go_to_init_state()\n",
    "                border_count = 0\n",
    "                \n",
    "            time_arr.append(time.time()-start_2)    \n",
    "        # Stop the actions\n",
    "        self.__env.stop()\n",
    "        \n",
    "        # Calculate the intermediate reward\n",
    "        if self.color_on:\n",
    "            reward = self.calculate_reward(state)\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        return state[-1]['index'], reward, done, {}\n",
    "      \n",
    "    def calculate_reward(self, state):\n",
    "        # Predict propba\n",
    "        if not self.on_field:\n",
    "          return -20\n",
    "        \n",
    "        weights = np.ones(shape=(self.num_measurements,))\n",
    "        weights = [weight * i for i, weight in enumerate(weights)]\n",
    "        x = np.array([s['raw_col'] for s in state]).squeeze()\n",
    "#         r = (np.argmax(self.reward_classifier.predict_proba(x), axis = 1) == 1).sum()\n",
    "        # sum the probabilities of black class and compute a function of it\n",
    "        black_proba = self.reward_classifier.predict_proba(x)[:,1]\n",
    "        black_proba_weighted = [weight * p for weight, p in zip(weights, black_proba)]\n",
    "      \n",
    "        black_threshold = 0.3\n",
    "        r = np.max([0, (np.sum(black_proba_weighted)-(black_threshold*self.num_measurements)) * 5])\n",
    "        return r\n",
    "    \n",
    "    def prepro(self,state):\n",
    "        # Deprecate this shit, preprocessing will be done in retrieving the get_state.\n",
    "        s = state[-1]\n",
    "        if self.color_on:\n",
    "            x = (s['cs'][0][0]//10,s['cs'][0][1]//10,s['cs'][0][2]//10, s['bot'][0]//36, s['top'][0]//36)\n",
    "        else:\n",
    "            x = (s['bot'][0]//36, s['top'][0]//36)\n",
    "        return x\n",
    "      \n",
    "    def get_state(self):\n",
    "        s_1 = self.state\n",
    "        s_2 = self.state\n",
    "        \n",
    "        s_2['bot'] = s_2['bot'][0]//36\n",
    "        s_2['top'] = s_2['top'][0]//36\n",
    "        \n",
    "        if self.color_on:\n",
    "            col = np.r_[s_1['cs'][0],s_2['cs'][0]]\n",
    "            col_ind = col//3\n",
    "            \n",
    "            s = {'index': (*tuple(col_ind),s_2['bot'], s_2['top']), 'raw_col' : np.array([col])}\n",
    "        else:\n",
    "            s = (s_2['bot'], s_2['top'])\n",
    "        return s\n",
    "      \n",
    "    @property\n",
    "    def state(self):\n",
    "      return self.__env.get_state()\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "      return len(self.__env.get_action_space()[0])\n",
    "      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = System(get_state_mode = 'dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sys.perform_actions([0,0])\n",
    "#sys.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', delta_measurement= 0.0, num_measurements = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 30\n",
    "\n",
    "# Make an Agent\n",
    "\n",
    "q_table = T_Agent(4, learn_rate = .8, gamma =.95)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Decay the exploration\n",
    "    q_table.explore_decay = i\n",
    "    \n",
    "    s = env.go_to_init_state()\n",
    "    rAll = 0\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "       \n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = q_table.next_action(s)\n",
    "        print('Action',a)\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        print('\\r   ', r)\n",
    "        \n",
    "        #Update Q-Table with new knowledge\n",
    "        q_table.update(r, s1)\n",
    "\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "\n",
    "    rewards.append(rAll)\n",
    "    print('#'*10, 'End Episode', '#'*10)\n",
    "    \n",
    "print(\"Average score over last part \" +  str(sum(rewards[-500:])/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(env.state)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_table.val_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpyc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.system import System\n",
    "from agent.tabular_q_learning import Agent as T_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- 0 (rotation), 1 (other rotation), 2 (move outwards), 3 (move inwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, field_classifier, reward_classifier, get_reward_function, get_state_function, state_queue_len=10):\n",
    "        self.system = System(brick_ip='ev3dev.local', get_state_mode='dict')\n",
    "        self.field_classifier = utils.load_pickle(field_classifier)\n",
    "        self.reward_classifier = utils.load_pickle(reward_classifier)\n",
    "        self.opposite_action = {0:1,1:0,2:3,3:2}\n",
    "        \n",
    "        self.on_field = True\n",
    "        self.border_count = 0\n",
    "#         self.color_on = color_on\n",
    "        \n",
    "        self.state_queue = collections.deque(maxlen=state_queue_len)\n",
    "        self.reward_queue = collections.deque(maxlen=state_queue_len)\n",
    "        \n",
    "        self.get_reward_function = get_reward_function\n",
    "        self.get_state_function = get_state_function\n",
    "        \n",
    "        for _ in range(state_queue_len):\n",
    "            self._new_state()\n",
    "        \n",
    "    def reset(self):\n",
    "        # stop current action\n",
    "        self.system.reset()\n",
    "        # Go to initial state\n",
    "\n",
    "        # return state\n",
    "#         return self.prepro([self.state])\n",
    "      \n",
    "    def go_to_init_state(self):\n",
    "        self.system.go_to_init_state()\n",
    "        print('#'*30)\n",
    "        print('Going to Init')\n",
    "        print('#'*30)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _color_from_one_state(s):\n",
    "        return s[:3]\n",
    "        \n",
    "    def _environment_checks(self):\n",
    "        # access color information from the last measurement \n",
    "        # according to self.new_state() ordering\n",
    "        color = np.array([self._color_from_one_state(s) for s in self.state_queue[-2]]).reshape(1,-1)\n",
    "        \n",
    "        if self.field_classifier.predict(color) == [0]:\n",
    "            print('I am outside')\n",
    "            border_count += 1\n",
    "\n",
    "            if self.on_field:\n",
    "                self.__env.perform_actions([self.opposite_action[action]])\n",
    "                print('BOUNCIN!!1')\n",
    "                time.sleep(1)\n",
    "            self.on_field = False\n",
    "        else:\n",
    "            self.on_field = True\n",
    "    \n",
    "        if border_count == 3:\n",
    "            self.go_to_init_state()\n",
    "            border_count = 0\n",
    "            \n",
    "    def _calculate_reward(self):\n",
    "        # Predict propba\n",
    "        if not self.on_field:\n",
    "            return -20\n",
    "        \n",
    "        weights = np.ones(shape=(self.state_queue_len,))\n",
    "        weights = [weight * i for i, weight in enumerate(weights)]\n",
    "        \n",
    "        colors = np.array([s[0] for s in self.state_queue]).squeeze()\n",
    "#         r = (np.argmax(self.reward_classifier.predict_proba(x), axis = 1) == 1).sum()\n",
    "        # sum the probabilities of black class and compute a function of it\n",
    "        black_proba = self.reward_classifier.predict_proba(colors[-2:])[:,1]\n",
    "        black_proba_weighted = [weight * p for weight, p in zip(weights, black_proba)]\n",
    "      \n",
    "        black_threshold = 0.3\n",
    "        r = np.max([0, (np.sum(black_proba_weighted)-(black_threshold*self.num_measurements)) * 5])\n",
    "        self.reward_queue.append(r)  \n",
    "        \n",
    "\n",
    "    def _new_state(self):\n",
    "        s = self.system.get_state()\n",
    "        color = s['cs'][0]\n",
    "        top_pos = s['top'][0]\n",
    "        bot_pos = s['bot'][0]\n",
    "        self.state_queue.append((*color, top_pos, bot_pos))\n",
    "        \n",
    "    def _cycle(self, action):\n",
    "        if action:\n",
    "            self.system.perform_actions([action])\n",
    "        \n",
    "        # gets new states and puts it in the queue\n",
    "        self._new_state()\n",
    "        \n",
    "        # environment specific checks like is it still in the field\n",
    "        self._environment_checks()\n",
    "        \n",
    "        # calculate the reward \n",
    "        self._calculate_reward()\n",
    "        \n",
    "    def step(self, action, free_cycles=5):\n",
    "        _cycle(action)  \n",
    "        for _ in range(free_cycles):\n",
    "            _cycle(None)\n",
    "        return state, reward, done, {}\n",
    "    \n",
    "    @property\n",
    "    def reward(self):\n",
    "        return self.get_reward_function(self.reward_queue)\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.get_state_function(self.state_queue)\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self.__env.get_action_space()[0])\n",
    "\n",
    "def get_reward_function(reward_queue):\n",
    "    return reward_queue[-1]\n",
    "    \n",
    "def get_state_function(state_queue):\n",
    "    print(state_queue)\n",
    "    state = list(state_queue)\n",
    "    s = state[-5:]\n",
    "    return np.array(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', \n",
    "                  state_queue_len = 10, \n",
    "                  get_reward_function = get_reward_function,\n",
    "                  get_state_function = get_state_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([(20, 18, 16, 0, 0), (20, 18, 16, 0, 0), (20, 18, 16, 0, 0), (20, 17, 16, 0, 0), (20, 17, 16, 0, 0), (20, 18, 16, 0, 0), (20, 18, 16, 0, 0), (20, 17, 16, 0, 0), (20, 18, 16, 0, 0), (20, 18, 16, 0, 0)], maxlen=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20, 18, 16,  0,  0],\n",
       "       [20, 18, 16,  0,  0],\n",
       "       [20, 17, 16,  0,  0],\n",
       "       [20, 18, 16,  0,  0],\n",
       "       [20, 18, 16,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Going to Init\n",
      "##############################\n",
      "Action 2\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    -20\n",
      "Action 2\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    6.504167575075455\n",
      "Action 2\n",
      "    10.483359129936186\n",
      "Action 3\n",
      "    10.485833436968495\n",
      "Action 1\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    0.0\n",
      "Action 3\n",
      "    0.0\n",
      "Action 1\n",
      "    10.269726900932097\n",
      "Action 0\n",
      "    9.322725240916746\n",
      "Action 0\n",
      "    10.46088582212191\n",
      "Action 1\n",
      "    10.473609233489391\n",
      "Action 1\n",
      "    0.0\n",
      "Action 3\n",
      "    3.095531736002787\n",
      "Action 1\n",
      "    1.4847248880071218\n",
      "Action 3\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    -20\n",
      "Action 2\n",
      "I am outside\n",
      "    6.58298959270123\n",
      "Action 2\n",
      "    2.4619132366819727\n",
      "Action 1\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    3.6989835960472894\n",
      "Action 0\n",
      "    6.104230455648736\n",
      "Action 2\n",
      "    1.3949511052051944\n",
      "Action 3\n",
      "    0.0\n",
      "Action 2\n",
      "    3.7202256433307443\n",
      "Action 2\n",
      "I am outside\n",
      "BOUNCIN!!1\n",
      "    0.0\n",
      "Action 1\n",
      "    0.0\n",
      "Action 3\n",
      "Average score over last part 0.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 30\n",
    "\n",
    "# Make an Agent\n",
    "\n",
    "q_table = T_Agent(4, learn_rate = .8, gamma =.95)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "stop_flag = False\n",
    "for i in range(num_episodes):\n",
    "    # Decay the exploration\n",
    "    q_table.explore_decay = i\n",
    "    \n",
    "    s = env.go_to_init_state()\n",
    "    rAll = 0\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    try:\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = q_table.next_action(s)\n",
    "            print('Action',a)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            print('\\r   ', r)\n",
    "\n",
    "            #Update Q-Table with new knowledge\n",
    "            q_table.update(r, s1)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "#         stop_flag = True\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "    rewards.append(rAll)\n",
    "    print('#'*10, 'End Episode', '#'*10)\n",
    "    \n",
    "print(\"Average score over last part \" +  str(sum(rewards[-500:])/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(env.state)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4, 4, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_table.val_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

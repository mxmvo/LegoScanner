{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpyc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.system import System\n",
    "from agent.tabular_q_learning import Agent as T_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- 0 (rotation), 1 (other rotation), 2 (move outwards), 3 (move inwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, field_classifier, reward_classifier, get_reward_function, get_state_function, state_queue_len=10):\n",
    "        self.system = System(brick_ip='ev3dev.local', get_state_mode='dict')\n",
    "        self.field_classifier = utils.load_pickle(field_classifier)\n",
    "        self.reward_classifier = utils.load_pickle(reward_classifier)\n",
    "        self.opposite_action = {0:3,1:2,2:1,3:0}\n",
    "        self.opposite_action = {i: self.action_space-i-1 for i in range(self.action_space)}\n",
    "        \n",
    "        self.on_field = True\n",
    "        self.border_count = 0\n",
    "#         self.color_on = color_on\n",
    "        \n",
    "        self.state_queue = collections.deque(maxlen=state_queue_len)\n",
    "        self.reward_queue = collections.deque(maxlen=state_queue_len)\n",
    "        \n",
    "        self.get_reward_function = get_reward_function\n",
    "        self.get_state_function = get_state_function\n",
    "        \n",
    "        for _ in range(state_queue_len):\n",
    "            self._new_state()\n",
    "        \n",
    "    def reset(self):\n",
    "        # stop current action\n",
    "        self.system.reset()\n",
    "        # Go to initial state\n",
    "\n",
    "        # return state\n",
    "#         return self.prepro([self.state])\n",
    "      \n",
    "    def go_to_init_state(self):\n",
    "        self.system.go_to_init_state()\n",
    "        print('#'*30)\n",
    "        print('Going to Init')\n",
    "        print('#'*30)\n",
    "        time.sleep(3)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _color_from_one_state(s):\n",
    "        return s[:3]\n",
    "        \n",
    "    def _environment_checks(self):\n",
    "        # access color information from the last measurement \n",
    "        # according to self.new_state() ordering\n",
    "        color = self._color_state_for_classifier\n",
    "        \n",
    "        if self.field_classifier.predict(color) == [0]:\n",
    "            print('I am outside')\n",
    "            self.border_count += 1\n",
    "\n",
    "            if self.on_field:\n",
    "                self.system.perform_actions([self.opposite_action[a] for a in self.current_action])\n",
    "                print('BOUNCIN!!1')\n",
    "                time.sleep(1)\n",
    "            self.on_field = False\n",
    "        else:\n",
    "            self.on_field = True\n",
    "    \n",
    "        if self.border_count == 3:\n",
    "            self.go_to_init_state()\n",
    "            self.border_count = 0\n",
    "            \n",
    "    @property\n",
    "    def _color_state_for_classifier(self):\n",
    "        return np.array([self._color_from_one_state(s) for s in list(self.state_queue)[-2:]]).reshape(1,-1)\n",
    "            \n",
    "    def _new_reward_approx(self):\n",
    "        def transform_proba_into_reward_approx(proba):\n",
    "            return np.max([0., 5. * (proba - 0.3)])\n",
    "            \n",
    "        # Predict propba\n",
    "        if not self.on_field:\n",
    "            return -10\n",
    "        \n",
    "        colors = self._color_state_for_classifier\n",
    "#         r = (np.argmax(self.reward_classifier.predict_proba(x), axis = 1) == 1).sum()\n",
    "        # sum the probabilities of black class and compute a function of it\n",
    "        black_proba = self.reward_classifier.predict_proba(colors)[:,1][0]\n",
    "        \n",
    "        self.reward_queue.append(transform_proba_into_reward_approx(black_proba))\n",
    "        \n",
    "    def _new_state(self):\n",
    "        s = self.system.get_state()\n",
    "        color = s['cs'][0]\n",
    "        top_pos = s['top'][0]\n",
    "        bot_pos = s['bot'][0]\n",
    "        self.state_queue.append((*color, top_pos, bot_pos))\n",
    "        \n",
    "    def _cycle(self, is_free_cycle):\n",
    "        if not is_free_cycle:\n",
    "            print(\"Performing action\", self.current_action)\n",
    "            self.system.perform_actions(self.current_action)\n",
    "        \n",
    "        # gets new states and puts it in the queue\n",
    "        self._new_state()\n",
    "        \n",
    "        # environment specific checks like is it still in the field\n",
    "        self._environment_checks()\n",
    "        \n",
    "        # calculate the reward \n",
    "        self._new_reward_approx()\n",
    "        \n",
    "    def step(self, action, free_cycles=5):\n",
    "        self.current_action = action\n",
    "        self._cycle(False)  \n",
    "        for _ in range(free_cycles):\n",
    "            self._cycle(True)\n",
    "        return self.state, self.reward, False, {}\n",
    "    \n",
    "    @property\n",
    "    def reward(self):\n",
    "        return self.get_reward_function(self.reward_queue)\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.get_state_function(self.state_queue)\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self.system.get_action_space()[0])\n",
    "\n",
    "def get_reward_function(reward_queue):\n",
    "    # how many last reward_approx to take into consideration\n",
    "    rewards = list(reward_queue)[-3:]\n",
    "    \n",
    "    # linear weights\n",
    "    weights = np.ones(shape=(len(rewards),))\n",
    "    weights = [weight * i for i, weight in enumerate(weights)]\n",
    "    rewards_weighted = [weight * p for weight, p in zip(weights, rewards)]\n",
    "    \n",
    "    # custom function deciding what should be rewarded (because reward_approx is based on probability)\n",
    "#     black_threshold = 0.3\n",
    "#     r = np.max([0, (np.sum(rewards_weighted)-(black_threshold*len(rewards_weighted))) * 5])\n",
    "    \n",
    "    return np.mean(rewards_weighted)\n",
    "    \n",
    "def get_state_function(state_queue):\n",
    "#     print(state_queue)\n",
    "    state = list(state_queue)\n",
    "    s = state[-5:]\n",
    "    return np.array(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensors': {'bot': <environment.system.EnvSensor object at 0x000001D98F636978>, 'top': <environment.system.EnvSensor object at 0x000001D98F636710>, 'cs': <environment.system.EnvSensor object at 0x000001D98F636748>}, 'actionables': {'bot': <environment.system.EnvActionable object at 0x000001D98F636940>, 'top': <environment.system.EnvActionable object at 0x000001D98F6369B0>}}\n"
     ]
    }
   ],
   "source": [
    "env = Environment('./mlp_on_off.pickle','./mlp_white_black.pickle', \n",
    "                  state_queue_len = 10, \n",
    "                  get_reward_function = get_reward_function,\n",
    "                  get_state_function = get_state_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.go_to_init_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing action [2, 2]\n",
      "(array([[288, 258, 268, -15,  -8],\n",
      "       [286, 255, 265, -22, -14],\n",
      "       [286, 254, 265, -28, -20],\n",
      "       [260, 207, 247, -34, -27],\n",
      "       [ 30,  25,  27, -40, -32]]), 0.8292632230868603, False, {})\n",
      "Performing action [0, 2]\n",
      "I am outside\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-162ac17977f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-7afacb7d9278>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, free_cycles)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfree_cycles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7afacb7d9278>\u001b[0m in \u001b[0;36m_cycle\u001b[1;34m(self, is_free_cycle)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# environment specific checks like is it still in the field\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_environment_checks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# calculate the reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7afacb7d9278>\u001b[0m in \u001b[0;36m_environment_checks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_field\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopposite_action\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_action\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BOUNCIN!!1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Desktop\\LEGO\\LegoScanner\\environment\\system.py\u001b[0m in \u001b[0;36mperform_actions\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mactionable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actionables'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mactionable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Desktop\\LEGO\\LegoScanner\\environment\\system.py\u001b[0m in \u001b[0;36mperform_action\u001b[1;34m(self, action_number)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mperform_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionable_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    a = np.random.randint(env.action_space, size = 2)\n",
    "    print(env.step(list(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.system.perform_actions([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "\n",
    "The agent takes as input a vector/matrix and output a probability distribution\n",
    "\n",
    "The action is taken using an argmax. Then reward is 1 or 0 then from the reward get the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_steps, features, num_actions, num_hidden = 5):\n",
    "        super(Agent, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_steps*num_actions, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.layer1(x))\n",
    "      x = F.softmax(self.layer2(x))\n",
    "      return x\n",
    "      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(num_steps = 3, features = 4, num_actions = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand = torch.rand(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3570, 0.2057, 0.1924, 0.2449], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 30\n",
    "\n",
    "# Make an Agent\n",
    "\n",
    "q_table = T_Agent(4, learn_rate = .8, gamma =.95)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "stop_flag = False\n",
    "for i in range(num_episodes):\n",
    "    # Decay the exploration\n",
    "    q_table.explore_decay = i\n",
    "    \n",
    "    s = env.go_to_init_state()\n",
    "    rAll = 0\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    try:\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = q_table.next_action(s)\n",
    "            print('Action',a)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            print('\\r   ', r)\n",
    "\n",
    "            #Update Q-Table with new knowledge\n",
    "            q_table.update(r, s1)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "#         stop_flag = True\n",
    "        env.reset()\n",
    "        break\n",
    "\n",
    "    rewards.append(rAll)\n",
    "    print('#'*10, 'End Episode', '#'*10)\n",
    "    \n",
    "print(\"Average score over last part \" +  str(sum(rewards[-500:])/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(env.state)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_table.val_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
